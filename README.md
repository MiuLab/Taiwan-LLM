# Language Models for Taiwanese Culture 


<p align="center">
âœï¸ <a href="https://huggingface.co/spaces/yentinglin/Taiwan-LLaMa2" target="_blank">Online Demo</a>  
â€¢
ğŸ¤— <a href="https://huggingface.co/yentinglin" target="_blank">HF Repo</a> â€¢ ğŸ¦ <a href="https://twitter.com/yentinglin56" target="_blank">Twitter</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/pdf/2305.13711.pdf" target="_blank">[Paper Coming Soon]</a>  
â€¢ ğŸ‘¨ï¸ <a href="https://yentingl.com/" target="_blank">Yen-Ting Lin</a> 
    <br/><br/>
    <img src="https://www.csie.ntu.edu.tw/~miulab/taiwan-llama/logo-v2.png" width="100"> <br/>
    <a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE">
<img src="https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg"></a>
    <a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE">
        <img src="https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg"></a>
    <br/>
   
</p>




## Overview
Taiwan-LLaMa is a full parameter fine-tuned model based on LLaMa 2 for Traditional Mandarin applications.

**Taiwan-LLaMa v1.0** pretrained on over 5 billion tokens and instruction-tuned on over 490k conversations both in traditional mandarin.

## Demo
A live demonstration of the model can be accessed at [Hugging Face Spaces](https://huggingface.co/spaces/yentinglin/Taiwan-LLaMa2).

## Key Features

1. **Traditional Mandarin Support**: The model is fine-tuned to understand and generate text in Traditional Mandarin, making it suitable for Taiwanese culture and related applications.

2. **Instruction-Tuned**: Further fine-tuned on conversational data to offer context-aware and instruction-following responses.

3. **Performance on Vicuna Benchmark**: Taiwan-LLaMa's relative performance on Vicuna Benchmark is measured against models like GPT-4 and ChatGPT. It's particularly optimized for Taiwanese culture.

4. **Flexible Customization**: Advanced options for controlling the model's behavior like system prompt, temperature, top-p, and top-k are available in the demo.


## Work in progress
- [ ] **Improved pretraining**: A refined pretraining process (e.g. more data from Taiwan, training strategies) is under development, aiming to enhance model performance for better Taiwanese culture.
- [ ] **Extend max length**: Utilizing the Rope mechanism as described in [the paper](https://arxiv.org/abs/2104.09864), the model's length will be extended from 4k to 8k.


## Taiwanese Culture Examples

| Question  | ChatGPT  | **Taiwan LLaMa v1.0**  | **Taiwan LLaMa v0.0 (w/o pretraining)**  | Llama2_Traditional_Chinese_13b_Chat  | 
|:-|:-|:-|:-|:-|
| ä»¥æƒ¡åœ°æ™¯è§€è‘—åçš„æ˜¯é«˜é›„å“ªä¸€å€‹æ™¯é»?   | ä»¥æƒ¡åœ°åœ°æ™¯è‘—åçš„æ˜¯é«˜é›„å¸‚çš„æ——æ´¥ã€‚    | é«˜é›„ä»¥æ“æœ‰æƒ¡åœ°æ™¯è§€è‘—åçš„æ™¯é»æ˜¯ç”°å¯®æœˆä¸–ç•Œã€‚âœ… | ä»¥æƒ¡åœ°æ™¯è§€è‘—åçš„æ˜¯é«˜é›„å¸‚çš„æ——æ´¥å³¶ã€‚æ——æ´¥å³¶æ˜¯ä¸€å€‹å°å³¶ï¼Œ... | é«˜é›„å¸‚å…§æœ‰å¤šå€‹ä»¥æƒ¡åœ°æ™¯è§€è‘—åçš„æ™¯é»ï¼Œå…¶ä¸­ä»¥ä¸‹é¢å¹¾å€‹æœ€ç‚ºè‘—åï¼š 1. é«˜é›„æ·é‹ç´…ç·šæ²¿é€”æ™¯é»ï¼šé«˜é›„å¸‚æ”¿åºœç‚ºäº†æ¨å»£é«˜é›„  |
| å”é³³æ˜¯èª°ï¼Ÿ  | å”é³³ï¼ˆAudrey Tangï¼‰æ˜¯å°ç£çš„æ”¿æ²»å®¶ã€ç¤¾æœƒæ´»å‹•å®¶å’Œè»Ÿé«”å·¥ç¨‹å¸«ã€‚å¥¹æ–¼2016å¹´æ“”ä»»å°ç£è¡Œæ”¿é™¢æ•¸ä½æ”¿å‹™å§”å“¡ï¼Œæ˜¯å°ç£æ”¿åºœæ­·å²ä¸Šé¦–ä½æ²’æœ‰æ”¿é»¨èƒŒæ™¯çš„æ”¿å‹™å®˜ã€‚å”é³³åœ¨è»Ÿé«”å·¥ç¨‹é ˜åŸŸæœ‰è±å¯Œçš„ç¶“é©—ã€‚ âœ… | å”é³³æ˜¯æ”¿æ²»è©•è«–å®¶ã€è³‡è¨Šç¤¾æœƒå­¸ç ”ç©¶è€…ï¼Œä¹Ÿæ˜¯å°ç£ç¬¬ä¸€ä½æ“”ä»»æ”¿åºœæ•¸ä½æ”¿å§”çš„äººã€‚ âœ… | å”é³³ï¼ˆTang Fengï¼‰æ˜¯ä¸€ä½è‘—åçš„ä¸­åœ‹æ¼”å“¡å’Œæ­Œæ‰‹ã€‚ä»–å‡ºç”Ÿæ–¼1974å¹´12æœˆ17æ—¥ï¼Œå‡ºç”Ÿåœ°é»åœ¨ä¸­åœ‹åŒ—äº¬ã€‚å”é³³åœ¨1995å¹´é–‹å§‹ä»–çš„æ¼”è—ç”Ÿæ¶¯ï¼Œä¸¦åœ¨åŒå¹´ç™¼è¡Œäº†ä»–çš„é¦–å¼µå°ˆè¼¯ã€Šæˆ‘ä¸å­¤å–®ã€‹ã€‚æ­¤å¾Œï¼Œä»–ç™¼è¡Œäº†è¨±å¤šå—æ­¡è¿çš„æ­Œæ›²ï¼Œä¸¦åœ¨å¤šéƒ¨é›»è¦–åŠ‡å’Œé›»å½±ä¸­æ“”ä»»ä¸»è§’ã€‚å”é³³å› å…¶ç²¾æ¹›çš„æ¼”æŠ€å’Œè¿·äººçš„æ­Œè²è€Œè¢«è­½ç‚ºâ€œäºæ´²é­…åŠ›ç”·ç‹â€ã€‚ä»–çš„éŸ³æ¨‚ç²å¾—äº†å¤šå€‹çé …ï¼ŒåŒ…æ‹¬â€œè¯è¡¨æœ€ä½³æ–°ç”·æ¼”å”±çâ€ã€â€œè¯è¡¨æœ€ä½³æ­Œæ›²çâ€å’Œâ€œè¯è¡¨æœ€ä½³èˆè¹ˆçâ€ç­‰ã€‚  | å”é³³æ˜¯å”æœæ™‚æœŸçš„ä¸€ä½è‘—åæ–‡å­¸å®¶ï¼Œä»–çš„ä»£è¡¨ä½œå“åŒ…æ‹¬ã€Šé™°é¨­ã€‹ã€ã€Šç‹‚å‡¡å­ã€‹ç­‰ã€‚ä»–çš„æ–‡å­¸é¢¨æ ¼å…·æœ‰å¼·çƒˆçš„å€‹æ€§å’Œå‰µæ„ï¼Œè¢«è­½ç‚ºã€Œå”ä»£æ–‡å­¸ä¹‹çˆ¶ã€ã€‚  |
| NTU åœ¨å“ª?                      | NTUä½æ–¼æ–°åŠ å¡ï¼Œå…·é«”åœ°å€æ˜¯50 Nanyang Avenue, Singapore 639798ã€‚   | NTU æ˜¯åœ‹ç«‹è‡ºç£å¤§å­¸çš„ç¸®å¯«ã€‚å®ƒæ˜¯è‡ºç£çš„ä¸€æ‰€é ‚å°–ç ”ç©¶å‹å¤§å­¸ï¼Œä½æ–¼è‡ºåŒ—å¸‚ã€‚ âœ…| NTU æ˜¯å°ç£çš„ä¸€æ‰€å¤§å­¸ï¼Œä½æ–¼å°åŒ—å¸‚ã€‚âœ… | NTU çš„ç¸½éƒ¨ä½æ–¼å°ç£æ–°åŒ—å¸‚æ¿æ©‹å€ï¼Œä½”åœ°é¢ç©ç´„ç‚º1,000å…¬é ƒã€‚|
| ç”šéº¼æ˜¯22K? | "22K" å¯èƒ½æŒ‡çš„æ˜¯é‡‘çš„ç´”åº¦ï¼Œå°¤å…¶æ˜¯åœ¨ç å¯¶å’Œé‡‘é£¾æ–¹é¢ã€‚ | "22K"æ˜¯æŒ‡å°ç£çš„å¤§å­¸ç•¢æ¥­ç”Ÿçš„èµ·è–ªï¼Œå¤§ç´„æ˜¯æ–°å°å¹£22,000å…ƒã€‚é€™å€‹è©è¢«ç”¨ä¾†å½¢å®¹å¤§å­¸ç•¢æ¥­ç”Ÿçš„èµ·è–ªåä½ï¼Œä¸¦ä¸”åœ¨å°±æ¥­å¸‚å ´ä¸Šå°æ–¼å¤§å­¸ç•¢æ¥­ç”Ÿçš„éœ€æ±‚ä¸è¶³ã€‚âœ…| 22Kæ˜¯æŒ‡æº«åº¦å°ºåº¦ä¸Šçš„22,000Kï¼ˆåƒåº¦ï¼‰ã€‚1Kä»£è¡¨ä¸€åº¦ï¼Œå³æº«åº¦å·®è·ï¼Œæ˜¯æ”æ°æº«æ¨™ï¼ˆæ”æ°åº¦ï¼‰çš„ä¸€ç™¾è¬åˆ†ä¹‹ä¸€ã€‚å› æ­¤ï¼Œ22Kç­‰æ–¼-253.15Â°Cï¼ˆæ”æ°æº«æ¨™ï¼‰æˆ–-423.67Â°Fï¼ˆè¯æ°æº«æ¨™ï¼‰ã€‚ |  22Kæ˜¯ä¸€ç¨®ç°¡å¯«ï¼Œå³22,000ã€‚å®ƒæ˜¯ä¸€å€‹æ•¸å­—ï¼Œè¡¨ç¤º22,000å€‹å–®ä½ã€‚|
| è‚‰åœ“è¦ä¸è¦åŠ é¦™èœï¼Ÿ                  | é€™æ˜¯å€‹å€‹äººå£å‘³çš„å•é¡Œï¼Œæœ‰äº›äººå–œæ­¡åœ¨è‚‰åœ“ä¸­åŠ å…¥é¦™èœï¼Œå› ç‚ºé¦™èœçš„æ¸…é¦™å¯ä»¥å¢æ·»é¢¨å‘³ï¼›è€Œæœ‰äº›äººå‰‡ä¸å–œæ­¡é¦™èœçš„å‘³é“ï¼Œè¦ºå¾—å®ƒæœƒè“‹éå…¶ä»–é£Ÿæçš„å‘³é“ã€‚æ‰€ä»¥è¦ä¸è¦åŠ é¦™èœï¼Œå¯ä»¥æ ¹æ“šå€‹äººå–œå¥½ä¾†æ±ºå®šã€‚ | é€™æ˜¯ä¸€å€‹å€‹äººå–œå¥½çš„å•é¡Œï¼Œæœ‰äº›äººå–œæ­¡åœ¨è‚‰åœ“è£¡åŠ é¦™èœï¼Œæœ‰äº›äººä¸å–œæ­¡ã€‚**åœ¨å°ç£**ï¼Œè‚‰åœ“åŠ é¦™èœæ˜¯ä¸€ç¨®å¸¸è¦‹çš„åšæ³•ï¼Œé€™æ˜¯ä¸€ç¨®åœ¨è‚‰åœ“ä¸ŠåŠ é¦™èœçš„åšæ³•ã€‚ç„¶è€Œï¼Œåœ¨å…¶ä»–åœ‹é€™æ˜¯ä¸€å€‹æ–‡åŒ–å·®ç•°çš„å•é¡Œã€‚ğŸ˜Š | è‚‰åœ“åŠ é¦™èœå¯ä»¥å¢æ·»ä¸€äº›èŠ³é¦™å’Œç¾å‘³çš„å£æ„Ÿï¼Œå¦‚æœä½ å–œæ­¡é¦™èœçš„å‘³é“ï¼Œå¯ä»¥è©¦è©¦åŠ å…¥è‚‰åœ“ä¸­ã€‚é¦™èœé€šå¸¸åœ¨è£½ä½œéç¨‹ä¸­ç‚’é¦™ï¼Œå¯ä»¥å¢æ·»ä¸€äº›èŠ³é¦™å’Œç¾å‘³çš„å‘³é“ã€‚å¦‚æœä½ æƒ³è¦åŠ å…¥é¦™èœï¼Œå¯ä»¥åœ¨è£½ä½œéç¨‹ä¸­ç‚’é¦™ä¸€äº›é¦™èœï¼Œç„¶å¾Œå°‡å…¶æ··å…¥è‚‰åœ“ä¸­ã€‚è‚‰åœ“åŠ é¦™èœå¯ä»¥å¢æ·»ä¸€äº›èŠ³é¦™å’Œç¾å‘³çš„å£æ„Ÿï¼Œå¦‚æœä½ å–œæ­¡é¦™èœçš„å‘³é“ï¼Œå¯ä»¥è©¦è©¦åŠ å…¥è‚‰åœ“ä¸­ã€‚| åŠ é¦™èœçš„è‚‰åœ“å‘³é“æ›´å¥½ï¼Œä½†æ˜¯ä¸åŠ é¦™èœçš„è‚‰åœ“ä¹Ÿå¾ˆå¥½åƒã€‚å¦‚æœä½ å–œæ­¡é¦™èœçš„å‘³é“ï¼Œé‚£éº¼åŠ ä¸Šé¦™èœå°±å¥½äº†ã€‚å¦‚æœä½ ä¸å–œæ­¡é¦™èœï¼Œé‚£éº¼ä¸åŠ ä¹Ÿä¸æœƒå½±éŸ¿è‚‰ |


## Model

We provide a number of model checkpoints that we trained. Please find them on Hugging Face [here](https://huggingface.co/models?search=taiwan-llama). Here are some quick links to the checkpoints that are finetuned from LLaMa 2:

| **Model**                                              | **13B**                                                                                                                       | 
|--------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **Taiwan-LLaMa v1.0** (_better for Taiwanese Culture_)   | ğŸ¤— <a href="https://huggingface.co/yentinglin/Taiwan-LLaMa-v1.0" target="_blank">yentinglin/Taiwan-LLaMa-v1.0</a>  | 
| Taiwan-LLaMa v0.9 (partial instruction set)            | ğŸ¤— <a href="https://huggingface.co/yentinglin/Taiwan-LLaMa-v0.9" target="_blank">yentinglin/Taiwan-LLaMa-v0.9</a>  | 
| Taiwan-LLaMa v0.0 (no Traditional Mandarin pretraining) | ğŸ¤— <a href="https://huggingface.co/yentinglin/Taiwan-LLaMa-v0.0" target="_blank">yentinglin/Taiwan-LLaMa-v0.0</a>  | 

## Data

ç¶“éå°ˆæ¥­å¾‹å¸«çš„æ³•å¾‹å»ºè­°ï¼Œæˆ‘å€‘æš«æ™‚å°‡æœ‰ç‰ˆæ¬Šç–‘æ…®çš„è³‡æ–™é›†ä¸‹æ¶ã€‚æˆ‘å€‘æœƒç©æ¥µå°‹æ±‚æ›´å¤šçš„æ³•å¾‹æ„è¦‹ï¼Œå¸Œæœ›èƒ½å„˜å¿«é‡æ–°ä¸Šæ¶é€™äº›è¨“ç·´è³‡æ–™é›†ã€‚

Due to legal concerns raised by our legal advisors regarding copyrighted material, we've temporarily removed the datasets. We're actively seeking further opinions and hope to make the training datasets available again soon.

| **Dataset**                     | **Link**                                                                                                                      | 
|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| ~~**Instruction-tuning**~~      | ğŸ¤— <a href="https://huggingface.co/datasets/yentinglin/traditional_mandarin_instructions" target="_blank">yentinglin/traditional_mandarin_instructions</a>                                           | 
| ~~Traditional Mandarin Pretraining~~ | ğŸ¤— <a href="https://huggingface.co/datasets/yentinglin/zh_TW_c4" target="_blank">yentinglin/zh_TW_c4</a>                                   | 

## Architecture
Taiwan-LLaMa is based on LLaMa 2, leveraging transformer architecture, <a href="https://github.com/Dao-AILab/flash-attention" target="_blank">flash attention 2</a>, and bfloat16.

It includes:

* Pretraining Phase: Pretrained on a vast corpus of over 5 billion tokens, extracted from common crawl in Traditional Mandarin.
* Fine-tuning Phase: Further instruction-tuned on over 490k multi-turn conversational data to enable more instruction-following and context-aware responses.

## Evaluating "Taiwan LLaMa" on TTQA (Taiwan Trivia Question Answering)

[Link to the Origianl TTQA dataset from MediaTek Research](https://github.com/mtkresearch/MR-Models/tree/main/datasets/TTQA)

[Link to the Taiwan LLaMa and ChatGPT Prediction on  TTQA](https://github.com/MiuLab/Taiwan-LLaMa/blob/main/evaluation/TTQA_1.0.0_tw_llama_v1.0.json)

Taiwan LLaMa is being benchmarked on the TTQA dataset, a evaluation set for question answering models specific to Taiwan-related trivia.

As of the current stage, Taiwan LLaMa has achieved a score identical to 75% (48 out of 64) of the performance observed with ChatGPT. (Both in 0-shot)

Further comparisons and in-depth analysis are in progress.


## Generic Capabilities on Vicuna Benchmark 

The data is translated into traditional mandarin for evaluating the general capability.


<img src="./images/zhtw_vicuna_bench_chatgptbaseline.png" width="700">

The scores are calculated with ChatGPT as the baseline, represented as 100%. The other values show the relative performance of different models compared to ChatGPT.

| Language Model                      | Relative Score (%) |
|-------------------------------------|--------------------|
| GPT-4                               | 102.59%            |
| ChatGPT                             | 100.00%            |
| **Taiwan-LLaMa v1.0**               | 76.76%             |
| Claude-Instant-1.2                  | 74.04%             |
| Llama2_Traditional_Chinese_13b_Chat | 56.21%             |




## How to deploy the model on my own machine?
We recommend hosting models with [ğŸ¤— Text Generation Inference](https://github.com/huggingface/text-generation-inference). Please see their [license](https://github.com/huggingface/text-generation-inference/blob/main/LICENSE) for details on usage and limitations.
```bash
bash run_text_generation_inference.sh "yentinglin/Taiwan-LLaMa-v1.0" NUM_GPUS DIR_TO_SAVE_MODEL PORT MAX_INPUT_LEN MODEL_MAX_LEN
```

Prompt format follows vicuna-v1.1 template:
```
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {user} ASSISTANT:
```

## Setup development environment
```bash
conda create -n taiwan-llama python=3.10 -y 
conda activate taiwan-llama
pip install -r requirements.txt
```

## å¸¸è¦‹å•é¡”

### ç¶²é Demoèˆ‡ç¨‹å¼ç¢¼åŸ·è¡Œçµæœæœ‰è½å·®

å¯ä»¥åƒè€ƒé€™å€‹ [#19 (comment)](https://github.com/your_repository/issues/19#issuecomment-YourCommentID)ã€‚

### èƒ½å¦å•†ç”¨åŒ–ï¼Ÿ

é—œæ–¼æ¨¡å‹èƒ½ä¸èƒ½å•†ç”¨ï¼Œæˆ‘å»ºè­°æ‚¨è‡ªè¡Œå°‹æ±‚æ³•å¾‹æ„è¦‹ã€‚

æ¨¡å‹ä½œè€… (Meta èˆ‡æˆ‘) éƒ½é¡˜æ„é–‹æ”¾å•†ç”¨ï¼Œä½†æ˜¯ã€å¯ä»¥å•†ç”¨çš„æ¨¡å‹â€è¨“ç·´åœ¨â€œæœ‰è‘—ä½œæ¬Šæ³•ä¿è­·çš„è³‡æ–™ä¸Šâ€ï¼Œæ˜¯å¦å¯ä»¥å•†ç”¨ã€éœ€è¦æ‚¨çš„åˆ¤æ–·ã€‚

å°ç£æ²’æœ‰ç›¸é—œæ³•æ¡ˆä¿è­·æ¨¡å‹è¨“ç·´åœ¨æœ‰è‘—ä½œæ¬Šçš„è³‡æ–™ä¸Šï¼Œä½†å°±æˆ‘çš„ç†è§£ï¼Œæˆ‘å€‘æ¨¡å‹é›–è¨“ç·´åœ¨è‘—ä½œæ¬Šè³‡æ–™ä¸Šï¼Œä½†ä¸¦æ²’æœ‰æŠ„è¥²è‘—ä½œæ¬Šäººçš„æ„æ€è¡¨ç¤ºï¼Œæ‰€ä»¥æ¨¡å‹æ˜¯å¯ä»¥å•†ç”¨çš„ã€‚

ä»¥ä¸Šæ˜¯æˆ‘è«®è©¢å¾‹å¸«çš„çµè«–ï¼Œç‚ºæ±‚è¬¹æ…é‚„è«‹æ‚¨å°‹æ±‚æ›´å°ˆæ¥­çš„æ³•å¾‹æ„è¦‹ã€‚

### è«‹å•è¨“ç·´æ­¤æ¨¡å‹æ™‚ä½¿ç”¨çš„æ©Ÿå™¨è¦æ ¼

Pretraining: 8 x A100 80G for 2 weeks  
Instruction finetuning: 8 x H100 for 12 hrs

#### English Version

### Web Demo and Code Execution Results Differ

Refer to this [#19 (comment)](https://github.com/your_repository/issues/19#issuecomment-YourCommentID).

### Can it be Commercialized?

For questions on commercial use, consult legal advice.

Both the model authors (Meta and I) are open to commercial use. However, whether a "commercially usable model" trained on "copyrighted data" can be used commercially is for you to decide.

To my understanding, although the model is trained on copyrighted data, it does not plagiarize. Therefore, it can be commercialized.

This is based on legal advice; for caution, consult a legal expert.

### Machine Specifications for Training This Model

Pretraining: 8 x A100 80G for 2 weeks  
Instruction finetuning: 8 x H100 for 12 hrs

## Citations
If you use our code, data, or models in your research, please cite this repository. You can use the following BibTeX entry:

```bibtex
@inproceedings{lin-chen-2023-llm,
    title = "{LLM}-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
    author = "Lin, Yen-Ting  and Chen, Yun-Nung",
    booktitle = "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.nlp4convai-1.5",
    pages = "47--58"
}

@misc{taiwanllama,
    author={Lin, Yen-Ting and Chen, Yun-Nung},
    title={Language Models for Taiwanese Culture},
    year={2023},
    url={https://github.com/MiuLab/Taiwan-LLaMa},
    note={Code and models available at https://github.com/MiuLab/Taiwan-LLaMa},
}
```

## Collaborate With Us
If you are interested in contributing to the development of Traditional Mandarin language models, exploring new applications, or leveraging Taiwan-LLaMa for your specific needs, please don't hesitate to contact us. We welcome collaborations from academia, industry, and individual contributors.

## License
The code in this project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

The models included in this project are licensed under the LLAMA 2 Community License. See the [LLAMA2 License](https://github.com/facebookresearch/llama/blob/main/LICENSE) for full details.

## OpenAI Data Acknowledgment
The data included in this project were generated using OpenAI's models and are subject to OpenAI's Terms of Use. Please review [OpenAI's Terms of Use](https://openai.com/policies/terms-of-use) for details on usage and limitations.


## Acknowledgements

We thank [Meta LLaMA team](https://github.com/facebookresearch/llama) and [Vicuna team](https://github.com/lm-sys/FastChat) for their open-source efforts in democratizing large language models.
